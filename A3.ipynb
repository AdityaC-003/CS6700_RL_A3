{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "\n",
    "# get locations of the colored states\n",
    "env = gym.make('Taxi-v3')\n",
    "RED_LOC, GREEN_LOC, YELLOW_LOC, BLUE_LOC = env.unwrapped.locs\n",
    "env.close()\n",
    "\n",
    "GRID_SIZE = 25\n",
    "N_PRIMITIVE_MOVES = 4\n",
    "N_PRIMITIVE_ACTIONS = N_PRIMITIVE_MOVES + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedyPolicy(q_value_arr, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, len(q_value_arr) - 1)\n",
    "    else:\n",
    "        return np.argmax(q_value_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        terminal_state: int,\n",
    "        gamma: float = GAMMA,\n",
    "        alpha: float = ALPHA,\n",
    "    ):\n",
    "        self.q_value = np.zeros((state_size, action_size))\n",
    "        self.terminal_state = terminal_state\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def get_action(self, state: int, epsilon: float = 0.1):\n",
    "        return epsilonGreedyPolicy(self.q_value[state], epsilon)\n",
    "\n",
    "    def q_update(self, state: int, action: int, reward: float, next_state: int):\n",
    "        self.q_value[state, action] += self.alpha * (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.q_value[next_state])\n",
    "            - self.q_value[state, action]\n",
    "        )\n",
    "\n",
    "    def check_done(self, state: int):\n",
    "        return state == self.terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRL:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        gamma: float = GAMMA,\n",
    "        alpha: float = ALPHA,\n",
    "    ):\n",
    "        self.q_values = np.zeros((state_size, action_size))\n",
    "        self.update_freq = np.zeros((state_size, action_size))\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_primitive(self, state: int, action: int, reward: float, next_state: int):\n",
    "\n",
    "        self.q_values[state, action] += self.alpha * (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.q_values[next_state])\n",
    "            - self.q_values[state, action]\n",
    "        )\n",
    "        self.update_freq[state, action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMDP_QLearning(HRL):\n",
    "\n",
    "    def __init__(self, state_size, action_size, gamma=GAMMA, alpha=ALPHA):\n",
    "        super().__init__(state_size, action_size, gamma, alpha)\n",
    "        self.options = []\n",
    "        for loc in [RED_LOC, GREEN_LOC, YELLOW_LOC, BLUE_LOC]:\n",
    "            self.options.append(Option(GRID_SIZE, N_PRIMITIVE_MOVES, loc))\n",
    "\n",
    "    # def update_option_midway(\n",
    "    #     self, state: int, action: int, reward: float, next_state: int\n",
    "    # ):\n",
    "    #     return\n",
    "\n",
    "    def update_option_end(\n",
    "        self, state: int, action: int, reward: float, next_state: int, opt_duration: int\n",
    "    ):\n",
    "        self.q_values[state, action] += self.alpha * (\n",
    "            reward\n",
    "            + (self.gamma**opt_duration) * np.max(self.q_values[next_state])\n",
    "            - self.q_values[state, action]\n",
    "        )\n",
    "        self.update_freq[state, action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, hrl):\n",
    "        self.env = env\n",
    "        self.hrl = hrl\n",
    "\n",
    "    def train(self, num_episodes: int = 1000):\n",
    "\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            # while episode is not over\n",
    "            while not done:\n",
    "\n",
    "                # choose action\n",
    "                action = epsilonGreedyPolicy(self.hrl.q_values[state])\n",
    "\n",
    "                # if primitive action\n",
    "                if action < 4:\n",
    "                    next_state, reward, is_terminal, if_trunc, _ = self.env.step(action)\n",
    "                    done = is_terminal or if_trunc\n",
    "                    self.hrl.update_primitive(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "\n",
    "                # if option\n",
    "                else:\n",
    "\n",
    "                    Option = self.hrl.options[action - 4]\n",
    "                    opt_start_state = state\n",
    "                    opt_reward, opt_duration, opt_done = 0, 0, False\n",
    "\n",
    "                    while not opt_done and not done:\n",
    "\n",
    "                        # choose action\n",
    "                        opt_action = Option.get_action(state)\n",
    "\n",
    "                        # take action\n",
    "                        next_state, reward, is_terminal, if_trunc, _ = self.env.step(\n",
    "                            opt_action\n",
    "                        )\n",
    "                        done = is_terminal or if_trunc\n",
    "\n",
    "                        # update option's q-values\n",
    "                        Option.q_update(state, opt_action, reward, next_state)\n",
    "\n",
    "                        # update reward\n",
    "                        opt_reward += reward * (self.hrl.gamma**opt_duration)\n",
    "\n",
    "                        # update q-values of hrl\n",
    "                        # self.hrl.update_option_midway(\n",
    "                        #     state, opt_action, reward, next_state\n",
    "                        # )\n",
    "\n",
    "                        # update duration\n",
    "                        opt_duration += 1\n",
    "\n",
    "                        # update state\n",
    "                        state = next_state\n",
    "\n",
    "                        # check if option is done\n",
    "                        opt_done = Option.check_done(state)\n",
    "\n",
    "                    # update initial state-action pair, if needed\n",
    "                    self.hrl.update_option_end(\n",
    "                        opt_start_state, action, opt_reward, next_state, opt_duration\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
